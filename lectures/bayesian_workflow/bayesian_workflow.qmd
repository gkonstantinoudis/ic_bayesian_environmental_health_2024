---
title: "Bayesian workflow"
subtitle: "Good practice of Bayesian inference"
author:
 - name: "Elizaveta Semenova"
   email: "elizaveta.p.semenova@gmail.com"
institute: "Department of Epidemiology and Biostatistics, Imperial College London"
date: 2024-08-21
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/bmeh_normal.png"
  data-background-size: 80%
  data-background-position: 60% 120%
format:
  revealjs:
    slide-number: true
    incremental: true
    chalkboard:
      buttons: false
      preview-links: auto
    logo: "../../assets/bmeh_normal.png"
    theme: [default, ../../assets/style.scss]
---

# Outline

- Bayesian inference recap: What? Why? How?
- What is a Bayesian workflow and why do we need it?
- Principles of Bayesian workflow
- Modern Bayesian workflow

# Bayesian inference

## Bayes formula:

 $$p(\theta|y) = \frac{p(y | \theta) p(\theta)}{p(y)}$$
Can you recall what the components of the Bayes formula are?

![](assets/bmeh_normal.png){.r-stretch}

## Bayes formula:

 $$p(\theta|y) = \frac{p(y | \theta) p(\theta)}{p(y)}$$
Can you recall what the components of the Bayes formula are?

![](assets/bmeh_normal.png){.r-stretch}

- $p(\theta)$ is the *prior* distribution, i.e. what is known *a priori*
- $p(y|\theta)$ is the *likelihood*, i.e. probability of observing the data given parameters $\theta$
- $p(\theta|y)$ is the *posterior* distribution, i.e. the distribution of parameters of interest after data were observed


## Bayes rule:

$$\underbrace{p(\theta|y)}_\text{posterior} \propto \underbrace{p(y | \theta)}_{\text{likelihood}}  \underbrace{p(\theta)}_{\text{prior}}$$

. . .


What can possibly go wrong?

. . .

A lot can go wrong!


## General principle of Bayesian inference:

- Specify a complete Bayesian model:
  - specify likelihood
  - specify priors for all parameters (saying "I don't know" is also a prior)

## General principle of Bayesian inference:

- Example:
  - consider **data** $y = \{ y_1, ..., y_n\}$
  - specify an **observation model**, e.g. $$p(y|\theta) = \prod_i N(y_i | \theta, \sigma^2)$$
   <!-- - here $\theta$ is a parameter which we want to infer-->
  - complete the model with a prior distribution, e.g. $$p(\theta)=N(0,1)$$


## General principle of Bayesian inference:

- Specify a complete Bayesian model
- Sample the posterior distribution of the parameter $\theta$.

. . .

Sometimes posterior is available in a closed form.

. . .

But rarely.

## Probabilistic programming languages

Probabilistic programming languages (PPLs) from a user's perspective:

- PPLs are designed to let the user **focus on modelling** while inference happens automatically.
- Users need to specify
  - prior,
  - likelihood.
- Inference is performed via powerful algorithms such as, for example, Markov Chain Monte Carlo (**MCMC**).
- Availability of **diagnostic tools**.

## Remark on inference methods

- Alongside "exact"methods such as MCMC, there also exist approximate methods, such as Variation Inference (VI).
- We are opting to use MCMC whenever possible since it has theoretical guarantees.

## Diagnosing MCMC outputs

We use multiple chains of MCMC to estimate the posterior:

. . .

<center>
![](assets/trace_theta.png){width=50%}

## Diagnosing MCMC outputs

- Convergence diagnostics
  - $\hat{R}$ statistic,
  - traceplots.
- Effective sample size (ESS):
  - samples will be typically autocorrelated within a chain, which increases the uncertainty of the estimation of posterior quantities,
  - ESS -- number of *independent* samples required to obtain the same level of uncertainty as from the available dependent samples.

## Diagnosing MCMC outputs

We use multiple chains of MCMC to inspect convergence after warm-up:

. . .

<center>
![](assets/trace_theta.png){width=50%}

## Diagnosing MCMC outputs

We use multiple chains and inspect convergence after warm-up:

. . .

<center>
![](assets/trace_theta_bad.png){width=50%}

...

## Diagnosing MCMC outputs

The post-warm-up samples of $\theta$ approximate its posterior distribution:

. . .

<center>
![](assets/post_theta.png){width=50%}

# Principles of Bayesian workflow

## Workflows as a 'good practice'

Workflows exist in a variety of disciplines. For example, in machine learning workflow standards are being formalised under the name of MLOps:

<center>
![](assets/mlops-loop-en.png){width=50%}


## Box's loop

In the 1960's, the statistician Box formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box's loop by Blei et. al. (2014):

<center>
![](assets/boxes_loop.png){width=60%}

## Modern Bayesian workflow

A systematic review of the steps within the modern Bayesian workflow, described in Gelman et al. (2020):

<center>
![](assets/bayes_workflow.png){width=40%}

## Prior predictive checks

**Prior predictive checking** consists in simulating data from the priors:

- visualize priors (especially after transformation),
- this shows the range of data compatible with the model,
- it helps understand the adequacy of the chosen priors, as it is often easier to elicit expert knowledge on measureable quantities of interest rather than abstract parameter values.


## Iterative model building

A possible realisation of the Bayesian workflow loop:

- Understand the **domain** and problem,
- Formulate the model **mathematically**,
- Implement model, test, **debug**,
- **debug, debug, debug**


## Iterative model building

- Understand the **domain** and problem,
- Formulate the model **mathematically**,
- Implement model, test, **debug**,
- Perform **prior predictive** check,
- Fit the model,
- Assess **convergence diagnostics**,
- Perform **posterior predictive** check,
- Improve the model **iteratively**: from baseline to complex and computationally efficient models.

# Examples

## Data
Assume that the true data comes from the model
$$y_i = a + b x_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2).$$

<center>
![](assets/data1.png){width=45%}

## Model
We implemented the model in our favourite PPL:

```R
code <- nimbleCode({
  for (i in 1:n) {
    y[i] ~ dnorm(a, sd = sigma)
  }
})
```

## Prior predictive check
Let us draw samples **from the priors**, i.e. we are **not using any data at this stage yet**, only trying to see what kind of data ($y$) this model is able to generate.

. . .

<center>
![](assets/data2.png){width=45%}


## Prior predictive check
Something doesn't look right...

. . .

<center>
![](assets/data2.png){width=45%}


## Debug
There was a bug in the model. Let's correct it:

```R
code <- nimbleCode({
  a ~ dnorm(0, sd = 100)
  b ~ dnorm(0, sd = 100)
  sigma ~ T(dnorm(0, sd = 10))

  for (i in 1:n) {
    y[i] ~ dnorm(a + b * x, sd = sigma)
  }
})
```

## Prior redictive check again
<center>
![](assets/data3.png){width=45%}

Better: now the range of prior predictive draws is covering the data.


## Fit the model
Print summary:
<center>
![](assets/fit1.png){width=60%}

## Plot estimates
<center>
![](assets/est1.png){width=45%}


## Convergence diagnostics

<center>
![](assets/fit1_conv.png){width=60%}


## Convergence diagnostics

Traceplots

<center>
![](assets/traceplots1.png){width=45%}

## Posterior predictive check

<center>
![](assets/posterior1.png){width=45%}

# Questions?
