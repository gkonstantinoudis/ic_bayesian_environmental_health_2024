---
title: "Bayesian workflow"
subtitle: "The 'How' of Bayesian inference"
author:
 - name: "Elizaveta Semenova"
   email: "elizaveta.p.semenova@gmail.com"
institute: "Computer Science Department, University of Oxford"
date: 2023-08-14
date-format: medium
title-slide-attributes:
  data-background-color: "#f3f4f4"
  data-background-image: "../../assets/bmeh_normal.png"
  data-background-size: 80%
  data-background-position: 60% 120%
format:
  revealjs:
    slide-number: true
    incremental: true
    chalkboard:
      buttons: false
      preview-links: auto
    logo: "../../assets/bmeh_normal.png"
    theme: [default, ../../assets/style.scss]
---

# Outline

-   Bayesian inference recap: What, Why, How
-   What is a workflow and why do we need it?
-   Principles of Bayesian workflow
-   Modern Bayesian workflow

# Bayesian inference

## Bayes formula:

$$ p(\theta|y) = \frac{p(y | \theta) p(\theta)}{p(y)}$$

## Bayes rule:

$$\underbrace{p(\theta|y)}_\text{Posterior} \propto \underbrace{p(y | \theta)}_{\text{Likelihood}}  \underbrace{p(\theta)}_{\text{Prior}}$$

. . .



What can possibly go wrong?

. . .

A lot can go wrong!


## General principle of Bayesian inference:

-   Specify a complete Bayesian model
    
## General principle of Bayesian inference:

-   Example:
    - consider <ins>data</ins> $y = \{ y_1, ..., y_n\}$
    - specify an <ins>observation model</ins>, e.g. $$p(y|\theta) = \prod_i N(y_i | \theta, 1)$$
   <!-- - here $\theta$ is a parameter which we want to infer-->
    - complete the model with a prior distribution, e.g. $$p(\theta)=N(0,1)$$


## General principle of Bayesian inference:

-   Specify a complete Bayesian model
-   Sample the posterior distribution of the parameter $\theta$.
  
. . .

Sometimes posterior is available in a closed form. 

. . .

But rarely.

## Probabilistic programming languages

Probabilistic programming languages (PPLs) from a user's perspective:

-   PPLs are designed to let the user <ins>focus on modelling</ins> while inference happens automatically.
-   Users need to specify
    -   prior,
    -   likelihood.
-   Inference is performed via powerful algorithms such as Markov Chain Monte Carlo (<ins>MCMC</ins>).
-   Availability of <ins>diagnostic tools</ins>.

## Remark on inference methods

-   Alongside "exact"methods such as MCMC, there also exist approximate methods, such as Variation Inference (VI). 
-   We are opting to use MCMC whenever possible since it has theoretical guarantees.


## Diagnosing MCMC outputs

We use multiple chains of MCMC to estimate the posterior

<center>
![](assets/trace_theta.png){width=50%}



## Diagnosing MCMC outputs

-   Convergence diagnostics
    -   $\hat{R}$ statistic,
    -   traceplots.
-   Effective sample size (ESS):
    -   samples will be typically autocorrelated within a chain, which increases the uncertainty of the estimation of posterior quantities
    -   ESS -- number of independent samples required to obtain the same level of uncertainty as from the available dependent samples

## Diagnosing MCMC outputs

We use multiple chains of MCMC to inspect convergence after warm-up

<center>
![](assets/trace_theta.png){width=50%} 



## Diagnosing MCMC outputs

We use multiple chains and inspect convergence after warm-up

<center>
![](assets/trace_theta_bad.png){width=50%} 



## Diagnosing MCMC outputs

The post-warm-up samples of $\theta$ approximate its posterior distribution

<center>
![](assets/post_theta.png){width=50%}

# Principles of Bayesian workflow

## Workflows as 'good practice' standards

Workflows exist in a variety of disciplines. For example, in machine learning workflow standards are being formalised under the name of MLOps:

<center>
![](assets/mlops-loop-en.png){width=50%}


## Box's loop

In the 1960's, the statistician Box formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box's loop by Blei et. al. (2014):

<center>
![](assets/boxes_loop.png){width=40%} 

## Modern Bayesian workflow

A systematic review of the steps within the modern Bayesian workflow, described in Gelman et al. (2020):

<center>
![](assets/bayes_workflow.png){width=40%} 

## Prior predictive checks

<ins>Prior predictive checking</ins> consists in simulating data from the priors:

-   visualize priors (especially after transformation)

-   this shows the range of data compatible with the model

-   it helps understand the adequacy of the chosen priors, as it is often easier to elicit expert knowledge on measureable quantities of interest rather than abstract parameter values


## Iterative model building

A possible realisation of the Bayeisan workflow loop:

-   Understand the <ins>domain</ins> and problem,

-   Formulate the model <ins>mathematically</ins>,

-   Implement model, test, <ins>debug</ins>,

-   <ins>debug, debug, debug</ins>


## Iterative model building

-   Understand the <ins>domain</ins> and problem,

-   Formulate the model <ins>mathematically</ins>,

-   Implement model, test, <ins>debug</ins>,

-   Perform <ins>prior predictive</ins> check,

-   Fit the model,

-   Assess <ins>convergence diagnostics</ins>,

-   Perform <ins>posterior predictive</ins> check,

-   Improve the model <ins>iteratively</ins>: from baseline to complex and computationally efficient models.

# Examples

## Data
(Let's generate some super simple data, i.e. linear regression. in 1d)

## Model: formulate and implement

## Prior predictive check
(chose some horrible priors here, e.g. for sigma)

## Prior predictive check
(change to a better prior)

## Prior redictive check
(chose some horrible priors here)

## Fit the model

## Convergence diagnostics
(show traceplots, R-hat, ESS)

## Posterior predictive

# References
