---
title: "Spatial and Spatio-temporal Modelling"
subtitle: "SHARP Bayesian Modeling for Environmental Health Workshop"
author: "Garyfallos Konstantinoudis"
date: "August 2023"
output: html_document
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(sf)
library(rgeos)
library(patchwork)
library(posterior)
library(bayesplot)
library(spdep)
library(lubridate)
library(fastDummies)
library(INLA)
library(coda)

extrafont::loadfonts()
theme_set(hrbrthemes::theme_ipsum())
knitr::opts_chunk$set(fig.align = "center")

set.seed(2)
```

## Goal of this computing lab session

In this practical you will be using the R software Statistical Package (www.r-project.org), as well as NIMBLE (https://r-nimble.org/) to carry out a disease mapping using study. You will use the COVID-19 deaths during March-July 2020, in England, at the LTLA geographical level (317 areas).

As part of this practical you will:

- Explore ways of visualizing spatial data
- Define the neighborhood matrix in R
- Fit and interpret the BYM model in NIMBLE
- Perform spatial ecological regression

## 1. Visualization of spatial areal data

```{r}
data_england <- read_sf(here("data", "England", "COVIDecoregression.shp"))
glimpse(data_england)
summary(data_england)
class(data_england)
```
The shapefile could be also imported using the function `readOGR` of the package `rgdal` (Geospatial Data Abstraction Library), such as `data_england <- readOGR(dsn = mypath, layer = "data_england")`, where the `dsn` argument specifies the data source name and `mypath` is the path where the file is stored. Then to convert `sp` object to `sf` object, we can use `st_as_sf` function, e.g. `data_england <- st_as_sf(data_england)`. This is not encouraged as the `rgdal` will be deprecated.


We can create basic maps of sf objects using the `plot()` function. Note that the following chunk takes a bit of time to run as it plots all the attributes of the `sf` object, plus the geometry of the `sf` object is pretty detailed (takes around 2-3 minutes):
```{r eval=FALSE}
# the default plot of an sf object is a multi-plot of all attributes

plot(data_england) # plot all the attributes
plot(data_england$geometry) # plot only the boundaries

```


We can do nicer plots using the package `ggplot2`. First let's reduce the resolution to get quicker maps.
```{r fig.width=3, fig.align='center'}
data_england_simpler <- gSimplify(as(data_england, "Spatial"), tol = 500)
data_england_simpler <- st_as_sf(data_england_simpler)
data_england_simpler <- cbind(data_england_simpler, data_england %>% mutate(geometry = NULL))

ggplot() +
  geom_sf(data = data_england_simpler, color = "red", fill = "white") +
  ggtitle("Map of LTLAs in England") +
  coord_sf() + # axis limits and CRS
  labs(x = "Longitude", y = "Latitude", fill = "") +
  theme_bw() + # dark-on-light theme
  theme(
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 14)
  )
```


And for nicer maps for the other columns:
```{r fig.width=11}
ggplot() +
  geom_sf(data = data_england_simpler, aes(fill = deaths)) +
  ggtitle("COVID-19 deaths") +
  theme_bw() +
  scale_fill_viridis_c(option = "A") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = expectd)) +
    ggtitle("Expected number of deaths") +
    theme_bw() +
    scale_fill_viridis_c(option = "D") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = deaths / expectd)) +
    ggtitle("Standardised mortality ratio") +
    theme_bw() +
    scale_fill_viridis_c(option = "B", name = "SMR")

# and the covariates
ggplot() +
  geom_sf(data = data_england_simpler, aes(fill = TtlICUB)) +
  ggtitle("Total ICU beds") +
  theme_bw() +
  scale_fill_viridis_c(option = "A") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = NO2)) +
    ggtitle("NO2") +
    theme_bw() +
    scale_fill_viridis_c(option = "D") |
  ggplot() +
    geom_sf(data = data_england_simpler, aes(fill = IMD)) +
    ggtitle("IMD") +
    theme_bw() +
    scale_fill_viridis_c(option = "B")
```

## 2. A model with an unstructed spatial component

The RRs will be smoothed using the Poisson-logNormal model. The inference is done with NIMBLE called through R.
In particular, let each area $i$ be indexed by  the integers $1, 2,...,N$. The model is as follows:

\begin{equation}
\begin{aligned}
O_{i}|\lambda_{i}  & \sim & \text{Poisson}(\lambda_{i}E_{i} ) \\
\log(\lambda_{i}) & = & \alpha + \theta_{i}\\
\theta_{i} & \sim & N(0,\sigma_{\theta})
\end{aligned}
\end{equation}

where $\sigma_{\theta}$ is a standard deviation term that controls the magnitude of $\theta_{i}$. We will first write the model in NIMBLE. Specify the prior of $\tau_{\theta}$ in the code below. You can try a Gamma with parameters 1 and 0.01
```{r eval=TRUE, echo = TRUE}
UnstrCode <- nimbleCode({
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood for observed counts
    log(mu[i]) <- log(E[i]) + alpha + theta[i]

    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE
    RR[i] <- exp(alpha + theta[i]) # area-specific RR
  }

  # Priors:
  alpha ~ dnorm(0, tau = 0.00001) # vague prior (small precision=large variance)
  overallRR <- exp(alpha) # overall RR across study region

  tau.theta ~ dgamma(1, 0.01) # prior for the precision hyperparameter
})
```

The following chunk of code subsets the data to London. The rest of the practical uses England as a whole. If you want quicker results,you can subset to London:
```{r eval=FALSE, warning = FALSE}
data_england[startsWith(data_england$LTLA, "E09"),] -> data_england_simpler
ggplot() + geom_sf(data = data_england_simpler, fill = "NA")
```

Create data object as required for NIMBLE
```{r eval=TRUE}
# Obtain the number of LTLAs
n.LTLA <- dim(data_england_simpler)[1]

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths # observed nb of deaths
)

COVIDConsts <- list(
  N = n.LTLA, # nb of LTLAs
  E = data_england_simpler$expectd # expected number of deaths
)
```


What are the parameters to be initialised? Create a list with two elements and call it `inits` (each a list) with different initial values for the parameters:
```{r eval=TRUE, echo=FALSE}
# Initialise the unknown parameters, 2 chains
inits <- list(
  list(alpha = 0.01, tau.theta = 10, theta = rep(0.01, times = n.LTLA)), # chain 1
  list(alpha = 0.5, tau.theta = 1, theta = rep(-0.01, times = n.LTLA)) # chain 2
) 
```


Set `params` a vector to monitor alpha, theta, tau.theta, overallRR, resRR, e and mu:
```{r eval=TRUE, echo = FALSE}
# Monitored parameters
params <- c("alpha", "theta", "tau.theta", "overallRR", "RR", "mu")
```
Note that the parameters that are not set, will NOT be monitored!

Specify the MCMC setting:
```{r eval=TRUE}
# MCMC setting
ni <- 50000 # nb iterations
nt <- 100 # thinning interval
nb <- 30000 # nb iterations as burn-in
nc <- 2 # nb chains
```

The burn-in should be long enough to discard the initial part of the Markov chains that have not yet converged to the stationary distribution.

Run the MCMC simulations calling Nimble from R using the function `nimbleMCMC()`
```{r eval=FALSE, message=FALSE, warning=FALSE}
t_0 <- Sys.time()
modelGS.sim <- nimbleMCMC(
  code = UnstrCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = params,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)
t_1 <- Sys.time()
t_1 - t_0 # ~ 2minutes
saveRDS(modelGS.sim, file = "NIMBLE_IDD_A1")
```

```{r echo = FALSE}
modelGS.sim <- readRDS("NIMBLE_IDD_A1")
```

Note that specifying `samplesAsCodaMCMC = FALSE`, the function `nimbleMCMC()` returns a list object (if `codaPkg = TRUE`, file names of NIMBLE output are returned for easy access by the coda package).

Summarize posteriors from `samples`:
```{r eval=TRUE, results="hide"}
head(modelGS.sim$summary$chain1, digits = 3)
head(modelGS.sim$summary$chain2, digits = 3)
head(modelGS.sim$summary$all.chains, digits = 3)
# also
modelGS.sim$summary$chain2[c(1, 2, 3, 7), ]
# or
modelGS.sim$summary$chain2["tau.theta", ]
```

You can produce the summary statistics of all the monitored parameters also typing
```{r eval=TRUE, results="hide"}
apply(modelGS.sim$samples$chain1, 2, mean)
apply(modelGS.sim$samples$chain1, 2, sd)
# also
mean(modelGS.sim$samples$chain1[, "tau.theta"])
sd(modelGS.sim$samples$chain1[, "tau.theta"])
```

You can also check the 95\% credible intervals of the posterior distribution of the overall relative risk and the 90\% credible intervals of the posterior distribution of $\tau$ hyperparameter
```{r eval=TRUE}
# 95% CI
quantile(modelGS.sim$samples$chain1[, "overallRR"],
  probs = c(0.025, 0.975)
)
# 90% CI
quantile(modelGS.sim$samples$chain1[, "tau.theta"],
  probs = c(0.05, 0.95)
)
```

### Check the convergence
We expect the chains to eventually converge to the stationary distribution. However, there is no guarantee that the chains have converged after a number of draws.
There is a combination of several standard ways to check convergence. Here some useful tools.

* *The Gelman-Rubin diagnostic (Rhat)*
The Gelman-Rubin diagnostic evaluates MCMC convergence by analyzing the difference between multiple Markov chains. The convergence is assessed by comparing the estimated between-chains and within-chain variances for each model parameter. Large differences between these variances indicate nonconvergence.

When the scale reduction factor is high (perhaps greater than 1.1), then we should run our chains out longer to improve convergence to the stationary distribution.

You can use the function `rhat()` in the package `posterior` to calculate the *Rhat* and combine it with the function `mcmc_rhat()` in the `bayesplot` package to get a graphical illustration of the *Rhats*. First you need to create a list of different matrices per parameter. The different columns of these matrices are the different chains:
```{r eval=TRUE, warning=FALSE}

# extract the parameters you want to estimate rhat for
params.rhat <- modelGS.sim$samples$chain1 %>% colnames()
params.rhat %>% head()

# create the list of matrices containing the different chains
lapply(params.rhat, function(Y) do.call(cbind, modelGS.sim$samples[,Y])) -> list.mat
list.mat[[1]] %>% head()

# calculate rhat
sapply(list.mat, posterior::rhat) -> rhat_values
rhat_values %>% head()

# plot the rhat
bayesplot::mcmc_rhat(rhat_values) + theme_bw() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

```

The *Rhat* of the overallRR is always lower than 1.1. We should also check the traceplots!

*Diagnostic plots*
The most straightforward approach for assessing convergence is based on simply plotting and inspecting traces, histograms, autocorrelation of the observed MCMC sample. For models with many parameters it is not practical to check the convergence for every parameter, so a choice should be made of the relevant parameters to monitor. In this way the convergence of the NIMBLE output can be assessed with a reasonable degree of confidence.

We can use the `bayesplot` package for diagnostics plots. Note that you need to access the samples of the `samples` object. You do this using the dollar sign:

Use the function `mcmc_trace()` to get the traceplot of the different chains for `tau.theta`:
```{r eval = TRUE, warning = FALSE}
mcmc_trace(modelGS.sim$samples, pars = c("tau.theta"))
```


Use the functions `mcmc_acf_bar()` and `mcmc_acf()` to get the autocorrelation plots for the different chains for `tau.theta`:
```{r eval = FALSE, warning=FALSE}
mcmc_acf_bar(modelGS.sim$samples, pars = c("tau.theta"))
mcmc_acf(modelGS.sim$samples, regex_pars = c("tau.theta"))
```

Use the functions `mcmc_hist()` and `mcmc_dens()` to get the histogram and the density plots of the posterior `tau.theta` for chain 1:
```{r eval = TRUE, fig.width=4, fig.height=3, warning = FALSE}

mcmc_hist(modelGS.sim$samples$chain1, pars = c("tau.theta"))
mcmc_dens(modelGS.sim$samples$chain1, pars = c("tau.theta"))
```

Use the function `mcmc_dens_overlay()` to get the density plot of the posterior `tau.theta` for both chains:
```{r eval = TRUE, fig.width=4, fig.height=3, warning = FALSE}
mcmc_dens_overlay(modelGS.sim$samples, pars = c("tau.theta"))
```

Retrieve the WAIC
```{r eval=TRUE}
modelGS.sim$WAIC
```

### Map of the (globally) smoothed RRs

* To map the smoothed RRs in R we extract the posterior mean of the relative risks
```{r eval=TRUE}
RR_COVID <- modelGS.sim$summary$all.chains[paste0("RR[", 1:n.LTLA, "]"), "Median"] # posterior median
```

* Add it on the shapefile
```{r eval=TRUE}
data_england_simpler$RR <- RR_COVID
```

* Using `ggplot2`, we can produce a map of the smoothed RRs
```{r eval=TRUE, fig.width=5, fig.align='center'} 

ggplot() + geom_sf(data = data_england_simpler, aes(fill = RR)) + theme_bw() + 
                     scale_fill_viridis_c(limits = c(0, 2), name = "SMR") 
```	


## 3. The BYM model

The RRs will be smoothed using the Poisson-logNormal model and the BYM model. The inference is done with NIMBLE called through R. In particular, let each area $i$ be indexed by  the integers $1, 2,...,N$. The model is as follows:

\begin{equation}
\begin{aligned}
\hbox{O}_i & \sim \hbox{Poisson}(E_i \lambda_i); \;\;\; i=1,...,N\\
\log \lambda_i & = \alpha + \theta_i + \phi_i\\
\theta_i &\sim \hbox{Normal}(0, \sigma^2_{\theta_i})\\
{\bf \phi} & \sim \hbox{ICAR}({\bf W}, \sigma_{\phi}^2) \,\, ,  \sum_i \phi_i  = 0 \\
\alpha & \sim \text{Uniform}(-\infty, +\infty) \\
1/\sigma_{\theta}^2 & \sim \hbox{Gamma}(0.5, 0.05) \\
1/\sigma_{\phi}^2 & \sim \hbox{Gamma}(0.5, 0.0005) \\
\end{aligned}
\end{equation}

where $\sigma_{\phi}$ is a standard deviation term that controls the magnitude of $\phi_{i}$, i.e. the spatial structured term.

```{r eval=TRUE, echo = TRUE}
BYMCode <- nimbleCode({
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood for observed counts
    log(mu[i]) <- log(E[i]) + alpha + theta[i] + phi[i]

    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE

    SMR[i] <- exp(alpha + theta[i] + phi[i])
    resRR[i] <- exp(theta[i] + phi[i]) # area-specific residual RR
    proba.resRR[i] <- step(resRR[i] - 1) # Posterior probability
  }

  # BYM prior
  phi[1:N] ~ dcar_normal(adj = adj[1:L], weights = weights[1:L], num = num[1:N], tau = tau.phi, zero_mean = 1)

  # Priors
  alpha ~ dflat() # vague prior (Unif(-inf, +inf))
  overallRR <- exp(alpha) # overall RR across study region

  tau.theta ~ dgamma(0.5, 0.05) # prior for the precision hyperparameter
  sigma2.theta <- 1 / tau.theta # variance of unstructured area random effects

  tau.phi ~ dgamma(0.5, 0.0005) # prior on precison of spatial area random effects
  sigma2.phi <- 1 / tau.phi # conditional variance of spatial area random effects
})
```


To fit a BYM model, we first need to define an adjacency matrix:

### Creating the adjacency matrix

To run the BYM model, the adjacency matrix needs to be provided. Recall that there are many ways of defining an adjacency matrix ${\bf W}$. Here we will use queen contiguity which is defined as:
\begin{equation}
w_{ij} =
\begin{cases}
1 & \text{if } j \in \partial_i  \\
0         & \text{otherwise}
\end{cases}
\end{equation}
where $\partial_i$ is the set of area adjacent to $i$, and $w_{ij}$ is the $ij$ element of ${\bf W}$.


### Adjacency matrix in R

Convert the polygons to a list of neighbors using the function `poly2nb()`.
```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
LTLA_nb <- poly2nb(pl = data_england_simpler)
LTLA_nb
# extract centroids from the England shp
centr <- st_centroid(data_england_simpler) %>% st_geometry()
# and plot the links
par(mar = c(0, 0, 0, 0))
plot(data_england_simpler$geometry, border = "grey66")
plot(LTLA_nb, centr, pch = 19, cex = .5, add = T)
```

Convert the list you defined previously to NIMLE format (i.e. a list of 3 components adj, num and weights) using the function `nb2WB()` and print a summary of the object.
```{r echo=TRUE, eval=TRUE}
nbWB_A <- nb2WB(nb = LTLA_nb)
names(nbWB_A)
# a list of three components is created:
# adj = ID for all the neighbors; weights = the weight for each neighbour; num: total nb of neighbors across the study region
```

Create data and constants objects as required for NIMBLE:
```{r eval=TRUE}
n.LTLA <- dim(data_england_simpler)[1]

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths # observed nb of deaths
)

COVIDConsts <- list(
  N = n.LTLA, # nb of LTLAs

  # adjacency matrix
  L = length(nbWB_A$weights), # the number of neighboring areas
  E = data_england_simpler$expectd, # expected number of deaths
  adj = nbWB_A$adj, # the elements of the neighbouring matrix
  num = nbWB_A$num,
  weights = nbWB_A$weights
)
```


Define the initial values for ALL the unknown parameters:
```{r echo=TRUE, eval=TRUE}
# initialise the unknown parameters, 2 chains
inits <- list(
  list(
    alpha = 0.01,
    tau.theta = 10,
    tau.phi = 1,
    theta = rep(0.01, times = n.LTLA),
    phi = c(rep(0.5, times = n.LTLA))
  ),
  list(
    alpha = 0.5,
    tau.theta = 1,
    tau.phi = 0.1,
    theta = rep(0.05, times = n.LTLA),
    phi = c(rep(-0.05, times = n.LTLA))
  )
)
```


Which model parameters do you want to monitor? Set these before running NIMBLE Call this object *params*.
```{r echo=TRUE, eval=TRUE}
params <- c("sigma2.theta", "sigma2.phi", "overallRR", "theta", "SMR", "resRR", "proba.resRR", "alpha")
```

Specify the MCMC setting
```{r echo=TRUE, eval=TRUE}
ni <- 50000 # nb iterations
nt <- 10 # thinning interval
nb <- 10000 # nb iterations as burning
nc <- 2 # nb chains
```


Run the MCMC simulations calling NIMBLE from R using the function `nimbleMCMC()`. If everything is specified reasonably, this needs approximately 2 minutes.
```{r echo=TRUE, eval=FALSE}
t0 <- Sys.time()
modelBYM.sim <- nimbleMCMC(
  code = BYMCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = params,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)
t1 <- Sys.time()
t1 - t0
saveRDS(modelBYM.sim, file = "NIMBLE_BYM_A3")
```

```{r echo=TRUE, eval=TRUE}
modelBYM.sim <- readRDS("NIMBLE_BYM_A3")
```

Retrieve WAIC and compare with previous model. Which model performs best?
```{r echo=TRUE, eval=TRUE}
modelBYM.sim$WAIC
```


Check convergence of resRR
```{r echo=TRUE, eval=TRUE, warning = FALSE}
mcmc_trace(modelBYM.sim$samples, pars = c("resRR[1]", "resRR[5]", "resRR[15]", "resRR[19]"))
```

Extract the residual RR (i.e. exp(V + U)) and the posterior probability that resRR is higher than 1 as. Create a `data.frame` for this purpose. Note that for the posterior probability, which is a vector of 0s and 1s we need to calculate the sum of 1s by the total sum, ie the mean.
```{r echo=TRUE, eval=TRUE}
RR_BYM <- data.frame(
  RR_BYM = modelBYM.sim$summary$all.chains[paste0("resRR[", 1:n.LTLA, "]"), "Median"],
  pp_BYM = modelBYM.sim$summary$all.chains[paste0("proba.resRR[", 1:n.LTLA, "]"), "Mean"]
)
```

Add a column ID for each LTLA, from 1 to `r n.LTLA`, to *RR_BYM*.
```{r echo=TRUE, eval=TRUE}
RR_BYM$LTLA <- data_england_simpler$LTLA
```

Change the name of the column corresponding to the posterior mean to BYM
```{r echo=TRUE, eval=TRUE}
colnames(RR_BYM) <- c("BYM", "pp_resRR", "LTLA")
```

Merge RR_BYM with COVID19Deaths using LTLA as column for merging. Call the new object *COVID19Deaths*:
```{r echo=TRUE, eval=TRUE}
COVID19Deaths <- left_join(data_england_simpler, RR_BYM, by = c("LTLA" = "LTLA"))
```

Map the smoothed residual RR (resRR). You can use the `plot` function or the other tools (`ggplot` or `tmap`) we used last week. Bonus question: Think of categorizing the continuous *BYM* column (an example could be quintiles) and plotting the categories.
```{r echo=TRUE, eval=TRUE, fig.width=10, warning=FALSE}
ggplot() +
  geom_sf(data = COVID19Deaths, aes(fill = BYM)) + # standard map
  scale_fill_viridis_c(name = "RR", limits = c(0, 2)) +
  ggtitle("Posterior median RR") +
  theme(
    axis.title.x = element_blank(), # removes xaxis title
    axis.title.y = element_blank() # removes yaxis title
  ) |
  ggplot() +
    geom_sf(data = COVID19Deaths, aes(fill = pp_resRR)) +
    scale_fill_viridis_c(name = "Prob") +
    ggtitle("Exceedance probability") +
    theme(
      axis.title.x = element_blank(), # removes xaxis title
      axis.title.y = element_blank() # removes yaxis title
    )
```

## 4. Ecological regression with the BYM model

Let $\mathcal{D}$ be the observation window of England and $A_1, A_2, \dots, A_N$ a partition denoting the LTLAs in England with $\cup_{i=1}^NA_i = \mathcal{D}$ and $A_i\cap A_j$ for every $i\neq j$. Let $O_1, O_2, \dots, O_N$ be the observed number of COVID-19 deaths occurred during March-July 2020 in England, $E_1, E_2, \dots, E_N$ is the expected number of COVID-19 deaths and $\lambda_1, \lambda_2, \dots, \lambda_N$ the standardized mortality ratio (recall $\lambda_i = \frac{O_i}{E_i}$). A standardized mortality ratio of $1.5$ implies that the COVID-19 deaths we observed in the $i$-th area are $1.5$ times higher to what we expected. Under the Poisson assumption we have:

  \begin{equation}
\begin{aligned}
\hbox{O}_i & \sim \hbox{Poisson}(E_i \lambda_i); \;\;\; i=1,...,N\\
\log \lambda_i & = \alpha +  \beta_1 X_{1i} + \beta_2 X_{2i} + \theta_i + \phi_i\\
\theta_i &\sim \hbox{Normal}(0, \sigma^2_{\theta_i})\\
{\bf \phi} & \sim \hbox{ICAR}({\bf W}, \sigma_{\phi}^2) \,\, ,  \sum_i \phi_i  = 0 \\
\alpha & \sim \text{Uniform}(-\infty, +\infty) \\
\beta_1, \beta_2 & \sim \mathcal{N}(0, 10) \\
1/\sigma_{\theta}^2 & \sim \hbox{Gamma}(0.5, 0.05) \\
1/\sigma_{\phi}^2 & \sim \hbox{Gamma}(0.5, 0.0005) \\
\end{aligned}
\end{equation}

the terms $\beta_1 X_{1i} + \beta_2 X_{2i} + \sum_{j=2}^5\beta_{3j} X_{3i}$, where $X_{1i}, X_{2i}, X_{3i}$ are the ICU beds, NO$_2$ and IMD in the $i$-th LTLA, $\beta_1, \beta_2, \sum_{j=2}^5\beta_{3j}$ the corresponding effects and $exp(\beta_1), exp(\beta_2)$ the relative risk of ICU beds or NO$_2$ for every unit increase and of the ICU beds or NO$_2$. For instance $exp(\beta_2) = 1.8$ means that for every unit increase of long term exposure to $NO_2$, the risk (read standardized mortality ratio) of COVID-19 deaths cancer increases by $80\%$. $exp(\beta_{32}), \beta_{33}, \beta_{34}, \beta_{35}$ are the relative risks compared to the baseline IMD category, ie the most deprived areas. An $exp(\beta_{35}) = 0.5$ means that the risk of COVID-19 deaths in most affluent areas decreases by $50%$ compared to the most deprived areas.$\tau_{\theta}$ is a precision (reciprocal of the variance) term that controls the magnitude of $\theta_{i}$. We will first write the model in NIMBLE.

```{r eval=TRUE, echo = TRUE}
BYMecoCode <- nimbleCode({
  for (i in 1:N) {
    O[i] ~ dpois(mu[i]) # Poisson likelihood for observed counts
    log(mu[i]) <- log(E[i]) + alpha + theta[i] + phi[i] + inprod(beta[], X[i,])
    # the inprod is equivalent with beta[1]*X1[i] + beta[2]*X2[i] + beta[3]*X32[i] + beta[4]*X33[i] + beta[5]*X34[i] + beta[6]*X35[i]
    
    SMR[i] <- alpha + theta[i] + phi[i] + inprod(beta[], X[i,])
    theta[i] ~ dnorm(0, tau = tau.theta) # area-specific RE
    resRR[i] <- exp(theta[i] + phi[i]) # area-specific residual RR
    proba.resRR[i] <- step(resRR[i] - 1) # Posterior probability
  }

  # BYM prior
  phi[1:N] ~ dcar_normal(adj = adj[1:L], weights = weights[1:L], num = num[1:N], tau = tau.phi, zero_mean = 1)

  # Priors
  alpha ~ dflat() # vague prior (Unif(-inf, +inf))
  overallRR <- exp(alpha) # overall RR across study region

  tau.theta ~ dgamma(0.5, 0.05) # prior for the precision hyperparameter
  sigma2.theta <- 1 / tau.theta # variance of unstructured area random effects

  tau.phi ~ dgamma(0.5, 0.0005) # prior on precison of spatial area random effects
  sigma2.phi <- 1 / tau.phi # conditional variance of spatial area random effects

  # priors for the fixed effects
  for(j in 1:K){
        beta[j] ~ dnorm(0, tau = 1)
      RR.beta[j] <- exp(beta[j])
  }
  
  RR.beta1_1NO2 <- exp(beta[1]/sd.no2) # get per 1 unit increase in the airpolution (scale back)
  
})
```


Create data object as required for NIMBLE
```{r eval=TRUE, warning=FALSE}
n.LTLA <- dim(data_england_simpler)[1]


# create the dummies for deprivation:
fastDummies::dummy_cols(data_england_simpler$IMD) %>% as_tibble() %>% 
  select(.data_1:.data_5) %>% 
  rename(IMD_1 = .data_1, IMD_2 = .data_2, IMD_3 = .data_3, 
         IMD_4 = .data_4, IMD_5 = .data_5) -> dummies
data_england_simpler <- cbind(data_england_simpler, dummies)

# Matrix of covariates
Xmat <- cbind(scale(data_england_simpler$NO2)[, 1],
            scale(data_england_simpler$TtlICUB)[, 1], 
            data_england_simpler$IMD_2, 
            data_england_simpler$IMD_3, 
            data_england_simpler$IMD_4,
            data_england_simpler$IMD_5)

# number of total covariates
K = ncol(Xmat)

# Format the data for NIMBLE in a list
COVIDdata <- list(
  O = data_england_simpler$deaths, # observed nb of deaths

  # covariates
  X = Xmat
)

COVIDConsts <- list(
  N = n.LTLA, # nb of LTLAs
  K = K,
  # adjacency matrix
  L = length(nbWB_A$weights), # the number of neighboring areas
  E = data_england_simpler$expectd, # expected number of deaths
  adj = nbWB_A$adj, # the elements of the neighboring matrix
  num = nbWB_A$num,
  weights = nbWB_A$weights, 
  
  sd.no2 = data_england_simpler$NO2 %>% sd()
)
```


Create the initial values for ALL the unknown parameters:
```{r echo=TRUE, eval=TRUE}
# initialise the unknown parameters, 2 chains
inits <- list(
  list(
    alpha = 0.01,
    beta = rep(0, K),
    tau.theta = 10,
    tau.phi = 1,
    theta = rep(0.01, times = n.LTLA),
    phi = c(rep(0.5, times = n.LTLA))
  ),
  list(
    alpha = 0.5,
    beta = rep(-1, K),
    tau.theta = 1,
    tau.phi = 0.1,
    theta = rep(0.05, times = n.LTLA),
    phi = c(rep(-0.05, times = n.LTLA))
  )
)
```


Which model parameters do you want to monitor? Set these before running NIMBLE Call this object *params*.
```{r echo=TRUE, eval=TRUE}
params <- c("sigma2.theta", "sigma2.phi", "overallRR", "theta", "beta", "RR.beta", "resRR", "proba.resRR", "alpha", "RR.beta1_1NO2")
```

Specify the MCMC setting
```{r echo=TRUE, eval=TRUE}
ni <- 50000 # nb iterations
nt <- 10 # thinning interval
nb <- 10000 # nb iterations as burning
nc <- 2 # nb chains
```


* Run the MCMC simulations calling NIMBLE from R using the function `nimbleMCMC()`. If everything is specified reasonably, this needs approximately 5 minutes.
```{r echo=TRUE, eval=FALSE}
t0 <- Sys.time()
modelBYMeco.sim <- nimbleMCMC(
  code = BYMecoCode,
  data = COVIDdata,
  constants = COVIDConsts,
  inits = inits,
  monitors = params,
  niter = ni,
  nburnin = nb,
  thin = nt,
  nchains = nc,
  setSeed = 9,
  progressBar = FALSE,
  samplesAsCodaMCMC = TRUE,
  summary = TRUE,
  WAIC = TRUE
)
t1 <- Sys.time()
t1 - t0
saveRDS(modelBYMeco.sim, file = "NIMBLE_BYM_A4")
```

```{r echo=TRUE, eval=TRUE}
modelBYMeco.sim <- readRDS("NIMBLE_BYM_A4")
```

Retrieve WAIC and compare with previous model. Which model performs best?
```{r echo=TRUE, eval=TRUE, warning=FALSE}
modelBYMeco.sim$WAIC
```

Check the convergence of the intercept and the different covariates. What do you observe?
```{r echo=TRUE, eval=TRUE, warning = FALSE, fig.height=8, fig.width=10}

mcmc_trace(modelBYMeco.sim$samples, pars = c("alpha", paste0("beta[", 1:K, "]")))

```

Retrieve summary statistics for the different covariates covariates and interpret (it is easier to interpret on the relative scale):
```{r echo=TRUE, eval=TRUE}
modelBYMeco.sim$summary$all.chains[paste0("RR.beta[", 1:K, "]"), ]
```

We can get a nice credible intervals plot as well:
```{r warning=FALSE}

modelBYMeco.sim$summary$all.chains[paste0("RR.beta[", 1:K, "]"), ] %>% 
  as_tibble() %>% 
  dplyr::select(Median, `95%CI_low`, `95%CI_upp`) %>% 
  mutate(covariate = 
           factor(c("NO2", "ICU", paste0("IMD", 2:5)), 
                  levels = c("NO2", "ICU", paste0("IMD", 2:5)))) -> cov.eff

cov.eff %>% head()

cov.eff %>% 
  ggplot() + 
  geom_point(aes(x = covariate, y = Median)) + 
  geom_errorbar(aes(x=covariate, ymin = `95%CI_low`, ymax = `95%CI_upp`), width = 0.2) + 
  ylim(c(0.5, 1.5)) + 
  geom_hline(yintercept = 1, lty = 2, col = "red")

```
The effect by unit increase in the long term exposure to NO$_2$ is `RR.beta1_1NO2`:
```{r}

# as relative risk per 1 unit increase in the long term NO2 exposure
modelBYMeco.sim$summary$all.chains[paste0("RR.beta1_1NO2"), c("Median", "95%CI_low", "95%CI_upp")] 

# as percentage increase in mortality for 1 unit increase in the long term NO2 exposure
(modelBYMeco.sim$summary$all.chains[paste0("RR.beta1_1NO2"), c("Median", "95%CI_low", "95%CI_upp")] - 1)*100
```
For every 1$\mu$g/$m^3$ increase in the long term exposure to NO$_2$ the COVID-19 mortality increase by 3\%.

Compare the BYM-related spatial field before and after adjusting for covariates.
```{r fig.width=10, warning=FALSE}
COVID19Deaths$BYM_ECO <- modelBYMeco.sim$summary$all.chains[paste0("resRR[", 1:n.LTLA, "]"), "Median"]

ggplot() +
  geom_sf(data = COVID19Deaths, aes(fill = BYM)) + # standard map
  scale_fill_viridis_c(name = "unadj", limits = c(0, 2.2)) +
  ggtitle("Posterior median RR (unadj)") +
  theme(
    axis.title.x = element_blank(), # removes xaxis title
    axis.title.y = element_blank() # removes yaxis title
  ) |
  ggplot() +
    geom_sf(data = COVID19Deaths, aes(fill = BYM_ECO)) +
    scale_fill_viridis_c(name = "adj", limits = c(0, 2.2)) +
    ggtitle("Posterior median RR (adj)") +
    theme(
      axis.title.x = element_blank(), # removes xaxis title
      axis.title.y = element_blank() # removes yaxis title
    )
```

## Closing remarks

In this lab session, we have explored how to fit spatial models using Bayesian regression in NIMBLE.
We looked at the two most common approaches: iid and the BYM model.

We used real data on COVID-19 deaths in England during March-July 2020 and first performed disease mapping to understand the spatial trends of COVID-19 mortality in the first stages of the pandemic. As part of this analysis, we visualised spatial data, fitted a model with an unstructured spatial random effect, demonstrated how to build a neighborhood matrix in R and fitted the BYM model. 

Subsequently, we examined the effect of long term exposure to NO$_2$ on COVID-19 mortality. We fitted a BYM model to account for unknown spatial confounding but in addition we accounted for total number of ICU beds and deprivation per LTLA. We reported evidence of an increased COVID-19 mortality for increasing levels of NO$_2$.

For more information on the final model see Konstantinoudis et al 2021 (10.1016/j.envint.2020.106316).

